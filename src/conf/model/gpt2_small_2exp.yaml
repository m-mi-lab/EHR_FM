# GPT2-Small with 2 experts MoE
# Base transformer config matches ETHOS ARES
# MoE config optimized based on best practices

# vocab_size is determined during runtime
n_positions: 2048      
n_embd: 768            
n_layer: 6            
n_head: 12              
# n_inner: null        # 4*n_embd if null
activation_function: "gelu" 

# Dropout - matches ETHOS
resid_pdrop: 0.3
embd_pdrop: 0.3
attn_pdrop: 0.3
bias: False            # lin layer bias, not gpt2 training bias

# MOE config - 2 experts with top_k=2 (all experts active)
n_experts: 2
top_k: 2               # Use both experts (recommended for 2-expert setup)
stride: 2              # Apply MoE every 2 layers
use_aux_loss: true
aux_loss_weight: 0.01
use_router_z_loss: false
router_z_loss_weight: 0.001
train_capacity: 2.0
eval_capacity: 2.5
min_capacity: 2

use_noisy_top_k: false
router_use_full_prec: false

# Distributed expert parallelism settings
expert_distributed: null  # null = auto-detect, true/false to override
allow_var_seq_len: false  # allow variable sequence lengths in distributed mode
expert_hidden_mult: 4     # hidden layer multiplier for experts
expert_mult_bias: true    # use multiplicative bias in GEGLU
expert_prenorm: false     # apply RMSNorm before expert
threshold_train: 0.2      # routing threshold during training
threshold_eval: 0.2       # routing threshold during eval
straight_through_dispatch_tensor: true  # use straight-through estimator for gradients