# MoE with 4 experts - training config
# Aligned with ETHOS hyperparameters (2026-02-06 fixes)
max_iters: 200000
eval_interval: 2000   
eval_iters: 200      # batches for evaluation
log_interval: 10       
save_checkpoints: True
checkpoint_interval: 1000  # Save periodic checkpoints 

resume_from_checkpoint: null

# Optimizer - matches ETHOS and monolith_million
lr: 6e-4             
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0         # 0.0 to disable 

# lr schedule
decay_lr: True         
warmup_iters: 5000     # Increased from 2000 for stability
lr_decay_iters: ${train.max_iters}
min_lr: 6e-5           # 10% of max_lr

# main - per-GPU batch size
batch_size: 16         # Per-GPU batch size
gradient_accumulation_steps: 16  # Increased for larger effective batch
# Effective batch per GPU: 16 × 16 = 256
# Total effective batch (2 GPUs): 16 × 16 × 2 = 512
dtype: 'bfloat16'      # Better for mixed precision training
num_workers: 0         # data loaders 
ddp_find_unused_params: False # if ddp says unused params, set to True
out_dir: ${hydra:run.dir}

