# Tiny config for quick testing and debugging
# Aligned with ETHOS hyperparameters but shorter training
max_iters: 10000       # Short run for testing
eval_interval: 500     # More frequent evals for testing
eval_iters: 50         # Fewer batches for quick eval
log_interval: 10       
save_checkpoints: True
checkpoint_interval: 500  # More frequent checkpoints for testing

resume_from_checkpoint: null

# Optimizer - same as production configs
lr: 6e-4             
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0         # 0.0 to disable 

# lr schedule
decay_lr: True         
warmup_iters: 500      # Shorter warmup for quick testing
lr_decay_iters: ${train.max_iters}
min_lr: 6e-5           # 10% of max_lr

# main - per-GPU batch size
batch_size: 16         # Per-GPU batch size
gradient_accumulation_steps: 4  # Smaller for faster iterations during testing
# Effective batch per GPU: 16 × 4 = 64
# Total effective batch (2 GPUs): 16 × 4 × 2 = 128
dtype: 'bfloat16'      # Same as production
num_workers: 0         # data loaders 
ddp_find_unused_params: False
out_dir: ${hydra:run.dir}

