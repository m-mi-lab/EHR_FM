# train loop params - FIXED VERSION
# This config fixes the critical hyperparameter issues in monolith_million.yaml
# Main changes: LR reduced from 1e-2 to 6e-4, batch size reduced from 128 to 64
max_iters: 1000000
eval_interval: 2000   
eval_iters: 200      # batches for evaluation
log_interval: 10       
save_checkpoints: True
checkpoint_interval: 1000  # Save periodic checkpoints every N iterations regardless of val loss 

resume_from_checkpoint: null

# Optimizer - FIXED: Learning rate reduced to match MoE models
lr: 6e-4              # CHANGED from 1e-2 (was 16.7x too high!)
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0         # 0.0 to disable 

# lr schedule - FIXED: min_lr and warmup adjusted
decay_lr: True         
warmup_iters: 2000     # INCREASED from 1000 for more stable warmup
lr_decay_iters: ${train.max_iters}
min_lr: 6e-5           # CHANGED from 1e-3 to match 10% of max LR

# main - FIXED: Batch size reduced for better generalization
batch_size: 64         # CHANGED from 128 (effective batch was 8x larger than MoE)
gradient_accumulation_steps: 4
dtype: 'bfloat16'      # 'float32', 'bfloat16', or 'float16'
num_workers: 0         # data loaders 
ddp_find_unused_params: False # if ddp says unused params, set to True
out_dir: ${hydra:run.dir}

