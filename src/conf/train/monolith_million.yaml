# train loop params
max_iters: 1000000
eval_interval: 2000   
eval_iters: 200      # batches for evaluation
log_interval: 10       
save_checkpoints: True
checkpoint_interval: 1000  # Save periodic checkpoints every N iterations regardless of val loss 

# resume_from_checkpoint: /workspace/outputs/2025-11-17/19-21-21_model=gpt2_small_monolith,train=monolith_million,world_size=auto/best_model.pt
resume_from_checkpoint: null

# Optimizer 
lr: 1e-2             
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0         # 0.0 to disable 

# lr
decay_lr: True         
warmup_iters: 1000    
lr_decay_iters: ${train.max_iters}
min_lr: 1e-3

# main
batch_size: 128 # individual_batch_size * gradient_accumulation_steps * num_gpus --- 4096, 16 if 1 expert
gradient_accumulation_steps: 4
dtype: 'float16'       # 'float32', 'bfloat16', or 'float16'
num_workers: 0      # data loaders 
ddp_find_unused_params: False # if ddp says unused params, set to True
out_dir: ${hydra:run.dir}

