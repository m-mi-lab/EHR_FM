{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d93288d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenize.constants import SpecialToken as ST\n",
    "from src.tokenize.datasets import TimelineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a1cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfe4117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimelineDataset(\n",
    "    input_dir=\"/workspace/ehr_stuff/EHR_FM/data/tokenized_datasets/mimic_train\",\n",
    "    n_positions=2048,\n",
    "    is_encoder_decoder=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3fcb73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = train_dataset.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d91bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = (len(vocab) // 64 + 1) * 64 if len(vocab) % 64 != 0 else len(vocab)\n",
    "tokens_of_interest = [ST.DEATH, ST.ADMISSION, ST.DISCHARGE]\n",
    "tokens_of_interest = {stoken: vocab.encode(stoken) for stoken in tokens_of_interest}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d34034d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<SpecialToken.DEATH: 'MEDS_DEATH'>: 438,\n",
       " <SpecialToken.ADMISSION: 'HOSPITAL_ADMISSION'>: 142,\n",
       " <SpecialToken.DISCHARGE: 'HOSPITAL_DISCHARGE'>: 141}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_of_interest # from admissions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e693cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(6 * 1_000_000)\n",
    "train_dataset, val_dataset = (\n",
    "    Subset(train_dataset, indices=indices)\n",
    "    for indices in torch.split_with_sizes(\n",
    "        torch.arange(len(train_dataset)), [len(train_dataset) - val_size, val_size]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb11a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_infinite_loader(loader):\n",
    "    while True:\n",
    "        yield from iter(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4bf7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "train_dataloader = make_infinite_loader(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6599dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2724d74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2048])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d81a9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.encode(\"MEDS_DEATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = make_infinite_loader(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "515dc584",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = len(val_dataset) // (32 * 2048) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e84df87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q1',\n",
       " 'LAB//50885//MG/DL',\n",
       " 'Q1',\n",
       " 'LAB//50893//MG/DL',\n",
       " 'Q2',\n",
       " 'LAB//50902//MEQ/L',\n",
       " 'Q4',\n",
       " 'LAB//50908//%',\n",
       " 'Q4',\n",
       " 'LAB//50910//IU/L',\n",
       " 'Q9',\n",
       " 'LAB//50911//NG/ML',\n",
       " 'Q7',\n",
       " 'LAB//50912//MG/DL',\n",
       " 'Q8',\n",
       " 'LAB//50931//MG/DL',\n",
       " 'Q8',\n",
       " 'LAB//50960//MG/DL',\n",
       " 'Q4',\n",
       " 'LAB//50970//MG/DL',\n",
       " 'Q10',\n",
       " 'LAB//50971//MEQ/L',\n",
       " 'Q10',\n",
       " 'LAB//50983//MEQ/L',\n",
       " 'Q1',\n",
       " 'LAB//50993//UIU/ML',\n",
       " 'Q9',\n",
       " 'LAB//51003//NG/ML',\n",
       " 'Q7',\n",
       " 'LAB//51006//MG/DL',\n",
       " 'Q10',\n",
       " 'LAB//51237//UNK',\n",
       " 'Q5',\n",
       " 'LAB//51274//SEC',\n",
       " 'Q7',\n",
       " 'LAB//51275//SEC',\n",
       " 'Q8',\n",
       " 'LAB//51221//%',\n",
       " 'Q4',\n",
       " 'LAB//51222//G/DL',\n",
       " 'Q3',\n",
       " 'LAB//51248//PG',\n",
       " 'Q3',\n",
       " 'LAB//51249//%',\n",
       " 'Q2',\n",
       " 'LAB//51250//FL',\n",
       " 'Q5',\n",
       " 'LAB//51265//K/UL',\n",
       " 'Q7',\n",
       " 'LAB//51277//%']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.decode(batch[0][0][200:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16b0e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab.decode(batch[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "685a8f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100,  ...,   24,  109,   59],\n",
       "        [-100, -100, -100,  ...,   73,   28,   74],\n",
       "        [-100, -100, -100,  ...,   31,   44,   27],\n",
       "        ...,\n",
       "        [-100, -100, -100,  ...,   34,   42,   30],\n",
       "        [-100, -100, -100,  ...,   35,   24,   96],\n",
       "        [-100, -100, -100,  ...,  119,   31,  155]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfaa688",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7a5ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=vocab_size,\n",
    "    n_positions=2048,\n",
    "    n_embd=64,\n",
    "    n_layer=1, ## change this stuff later if the model is bad\n",
    "    n_head=4,\n",
    "    n_inner=None,\n",
    "    activation_function=\"gelu\",\n",
    "    resid_pdrop=0, ## change this stuff later if the model is bad\n",
    "    embd_pdrop=0, ## change this stuff later if the model is bad\n",
    "    attn_pdrop=0, ## change this stuff later if the model is bad\n",
    "    bias=False, # model doesn't perform well without bias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2e36a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model.py\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from functools import lru_cache\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers.activations\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2Config\n",
    "\n",
    "ModelOutput = namedtuple(\"ModelOutput\", [\"loss\", \"logits\"])\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config, attention_weights: list | None = None):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.attn_pdrop\n",
    "        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n",
    "        if not self.flash or attention_weights is not None:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.tril(torch.ones(config.n_positions, config.n_positions)).view(\n",
    "                    1, 1, config.n_positions, config.n_positions\n",
    "                ),\n",
    "                persistent=False,\n",
    "            )\n",
    "        self.attention_weights = attention_weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the\n",
    "        # batch dim\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash and self.attention_weights is None:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q,\n",
    "                k,\n",
    "                v,\n",
    "                attn_mask=None,\n",
    "                dropout_p=self.dropout if self.training else 0,\n",
    "                is_causal=True,\n",
    "            )\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "            self.attention_weights.append(att.detach().cpu())\n",
    "        y = (\n",
    "            y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        )  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.activation = transformers.activations.get_activation(config.activation_function)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config, attention_weights: list | None = None):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config, attention_weights=attention_weights)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT2LMNoBiasModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GPT2Config,\n",
    "        return_attention=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.return_attention = return_attention\n",
    "        self.attention_weights = [] if return_attention else None\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe=nn.Embedding(config.n_positions, config.n_embd),\n",
    "                drop=nn.Dropout(config.embd_pdrop),\n",
    "                h=nn.ModuleList(\n",
    "                    [Block(config, self.attention_weights) for _ in range(config.n_layer)]\n",
    "                ),\n",
    "                ln_f=nn.LayerNorm(config.n_embd, bias=config.bias),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"c_proj.weight\"):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        pos = torch.arange(0, config.n_positions, dtype=torch.long)\n",
    "        self.register_buffer(\"pos\", pos, persistent=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    @lru_cache\n",
    "    def num_parameters(self, exclude_embeddings=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if exclude_embeddings:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def forward(self, input_ids, labels=None) -> ModelOutput:\n",
    "        _, t = input_ids.size()\n",
    "        if self.return_attention:\n",
    "            self.attention_weights.clear()\n",
    "\n",
    "        tok_emb = self.transformer.wte(input_ids)\n",
    "        pos_emb = self.transformer.wpe(self.pos[:t])\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if labels is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "\n",
    "        return ModelOutput(loss=loss, logits=logits)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_next_token(self, x: torch.Tensor, return_probs: bool = False, top_k: int | None = None):\n",
    "        logits = self(x).logits\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        if return_probs:\n",
    "            return next_token, probs\n",
    "        return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1934c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMNoBiasModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "879775dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMNoBiasModel(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(4480, 64)\n",
       "    (wpe): Embedding(2048, 64)\n",
       "    (drop): Dropout(p=0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (c_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (c_proj): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=64, out_features=4480, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5922d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# scaler\n",
    "scaler = torch.amp.GradScaler(\"float16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c545c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'configure_optimizers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# optimizer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m optimizer = \u001b[43mconfigure_optimizers\u001b[49m(\n\u001b[32m      3\u001b[39m     raw_model, cfg.weight_decay, cfg.lr, (cfg.beta1, cfg.beta2), device\n\u001b[32m      4\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'configure_optimizers' is not defined"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "optimizer = configure_optimizers(\n",
    "    raw_model, cfg.weight_decay, cfg.lr, (cfg.beta1, cfg.beta2), device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e454b967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb52eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8630d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenize.vocabulary import Vocabulary\n",
    "from src.tokenize.constants import SpecialToken as ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(\"/workspace/ehr_stuff/EHR_FM/data/tokenized_datasets/mimic_train/vocab_t4432.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37b22a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 64\n"
     ]
    }
   ],
   "source": [
    "vocab_size = (len(vocab) // 64 + 1) * 64 if len(vocab) % 64 != 0 else len(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2bb463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1b9083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_of_interest = [ST.DEATH, ST.ADMISSION, ST.DISCHARGE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4f4472f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<SpecialToken.DEATH: 'MEDS_DEATH'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokens_of_interest = {stoken: vocab.encode(stoken) \u001b[38;5;28;01mfor\u001b[39;00m stoken \u001b[38;5;129;01min\u001b[39;00m tokens_of_interest}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ehr_stuff/EHR_FM/src/tokenize/vocabulary.py:80\u001b[39m, in \u001b[36mVocabulary.encode\u001b[39m\u001b[34m(self, codes)\u001b[39m\n\u001b[32m     78\u001b[39m     codes = codes.tolist()\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(codes, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stoi[codes]\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.stoi[code] \u001b[38;5;28;01mfor\u001b[39;00m code \u001b[38;5;129;01min\u001b[39;00m codes]\n",
      "\u001b[31mKeyError\u001b[39m: <SpecialToken.DEATH: 'MEDS_DEATH'>"
     ]
    }
   ],
   "source": [
    "tokens_of_interest = {stoken: vocab.encode(stoken) for stoken in tokens_of_interest}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee919e69",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'MEDS_DEATH'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m vocab.encode(\u001b[33m\"\u001b[39m\u001b[33mMEDS_DEATH\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ehr_stuff/EHR_FM/src/tokenize/vocabulary.py:80\u001b[39m, in \u001b[36mVocabulary.encode\u001b[39m\u001b[34m(self, codes)\u001b[39m\n\u001b[32m     78\u001b[39m     codes = codes.tolist()\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(codes, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stoi[codes]\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.stoi[code] \u001b[38;5;28;01mfor\u001b[39;00m code \u001b[38;5;129;01min\u001b[39;00m codes]\n",
      "\u001b[31mKeyError\u001b[39m: 'MEDS_DEATH'"
     ]
    }
   ],
   "source": [
    "vocab.encode(\"MEDS_DEATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m vocab.stoi(\u001b[33m\"\u001b[39m\u001b[33mMEDS_DEATH\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "vocab.stoi(\"MEDS_DEATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e9ad38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MEDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
